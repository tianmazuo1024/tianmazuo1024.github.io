import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,o as i,d as e}from"./app-CiwSPZKD.js";const n={},l=e(`<h2 id="spark概述" tabindex="-1"><a class="header-anchor" href="#spark概述"><span>Spark概述</span></a></h2><p>如果说<a href="https://hadoop.apache.org" target="_blank" rel="noopener noreferrer">Hadoop</a>是用来做分布式数据存储的话，那么<a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a>就是是用来做分布式数据计算的。</p><p>虽然<a href="https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="noopener noreferrer">MapReduce</a>也是分布式数据计算引擎，但它基本上都是通过磁盘执行计算任务的。</p><p>而<a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a>完全是基于内存的计算引擎，所以它不仅可以完成和<a href="https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="noopener noreferrer">MapReduce</a>同样的离线计算任务，还能执行<code>实时</code>计算任务，以及和<a href="https://hive.apache.org/" target="_blank" rel="noopener noreferrer">Hive</a>类似的<code>SQL</code>查询。</p><p>所以<a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a>也被称为<code>统一计算引擎</code>（<a href="https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="noopener noreferrer">MapReduce</a>和<a href="https://hive.apache.org/" target="_blank" rel="noopener noreferrer">Hive</a>能做的它都能做）。</p><p><a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a>有以下几个显著的特点。</p><ul><li><p>既能做<code>批处理</code>（<code>离线计算</code>），也能做<code>流处理</code>（<code>实时计算</code>，<code>流处理</code>在后面的<a href="https://flink.apache.org/" target="_blank" rel="noopener noreferrer">Flink</a>部分会有深入分析）。</p></li><li><p>虽然<a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a>基于<a href="https://www.scala-lang.org/" target="_blank" rel="noopener noreferrer">Scala</a>语言开发，但可以使用多种语言来实现应用程序的快速开发，例如<a href="https://www.scala-lang.org/" target="_blank" rel="noopener noreferrer">Scala</a>、<a href="https://www.oracle.com/java/technologies/downloads/archive/" target="_blank" rel="noopener noreferrer">Java</a>和<a href="https://www.python.org/" target="_blank" rel="noopener noreferrer">Python</a>，而且可以实现<code>函数式编程</code> + <code>SQL</code>混合风格的开发。</p></li></ul><div class="language-scala line-numbers-mode" data-highlighter="shiki" data-ext="scala" data-title="scala" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">......</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">logs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> spark.read.json(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;log.json&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">logs.where(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;date &gt;= 2023-01-01 00:00:00&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).select(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;username&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">......</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">words.flatMap(_.split(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot; &quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)).map((_, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)).map(_._2).reduceLeft(_ </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">+</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> _)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">......</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>多种文件读取和任务执行方式。<a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a>的数据源可以是普通文本文件，可以是<a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_blank" rel="noopener noreferrer">HDFS</a>，或者<a href="https://en.wikipedia.org/wiki/NoSQL" target="_blank" rel="noopener noreferrer">NoSQL</a>中存储的数据，也可以是<a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer">Kafka</a>等其他几百个数据源中的数据。而它的大数据计算任务，不仅可以通过<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener noreferrer">YARN</a>来执行，也可以通过<a href="https://mesos.apache.org/" target="_blank" rel="noopener noreferrer">Mesos</a>和<a href="https://www.kubernetes.io/" target="_blank" rel="noopener noreferrer">K8S</a>执行。</p></li><li><p>通过它的几大核心组件：<a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="noopener noreferrer">Spark Streaming</a>、<a href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="noopener noreferrer">Spark SQL</a>、<a href="https://spark.apache.org/docs/latest/ml-guide.html" target="_blank" rel="noopener noreferrer">Spark MLib</a>和<a href="https://spark.apache.org/docs/latest/graphx-programming-guide.html" target="_blank" rel="noopener noreferrer">Spark GraphX</a>，实现一站式的大数据分析和处理。</p></li></ul><p>下面是<a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a>和<a href="https://hadoop.apache.org" target="_blank" rel="noopener noreferrer">Hadoop</a>的一个简单对比。</p><table><thead><tr><th style="text-align:center;"></th><th style="text-align:left;">Spark</th><th style="text-align:left;">Hadoop</th></tr></thead><tbody><tr><td style="text-align:center;">综合能力</td><td style="text-align:left;">所有的能力都与<code>计算</code>相关</td><td style="text-align:left;">既包括<a href="https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="noopener noreferrer">MapReduce</a>计算，也包括<a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_blank" rel="noopener noreferrer">HDFS</a>存储，还包括<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener noreferrer">YARN</a>资源管理和任务调度</td></tr><tr><td style="text-align:center;">计算模型</td><td style="text-align:left;">可以包含多个计算任务，实现复杂的迭代计算</td><td style="text-align:left;">仅包含<code>Map</code>和<code>Reduce</code>两个阶段，不够灵活</td></tr><tr><td style="text-align:center;">处理速度</td><td style="text-align:left;">计算任务在内存执行</td><td style="text-align:left;"><a href="https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="noopener noreferrer">MapReduce</a>基于磁盘运行</td></tr><tr><td style="text-align:center;">功能定位</td><td style="text-align:left;">通用计算引擎</td><td style="text-align:left;">大数据基础设施</td></tr></tbody></table><figure><img src="https://tianmazuo.com/technology/bigdata/spark/spark-01.png" alt="Spark + Hadoop" tabindex="0" loading="lazy"><figcaption>Spark + Hadoop</figcaption></figure><p><a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a>既可以独立运行，也可以在<a href="https://hadoop.apache.org" target="_blank" rel="noopener noreferrer">Hadoop</a>的<a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener noreferrer">YARN</a>中运行。</p><p>下面的部署都是按<code>独立运行</code>（也叫<code>Standalone</code>）的方式进行的。</p><br><h2 id="docker独立部署" tabindex="-1"><a class="header-anchor" href="#docker独立部署"><span>Docker独立部署</span></a></h2><p>安装好<a href="https://www.docker.com/" target="_blank" rel="noopener noreferrer">Docker</a>，然后执行下面的脚本。</p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" data-title="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">version:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &#39;3&#39;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">services:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">  spark-master:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    image:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> bde2020/spark-master:3.2.1-hadoop3.2</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    container_name:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> spark-master</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    ports:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">      -</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;8080:8080&quot;</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">      -</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;7077:7077&quot;</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    volumes:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">      -</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> /home/work/volumes/spark:/data</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    environment:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">      -</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> INIT_DAEMON_STEP=setup_spark</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">  spark-worker-1:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    image:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> bde2020/spark-master:3.2.1-hadoop3.2</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    container_name:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> spark-worker-1</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    depends_on:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">      -</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> spark-master</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    ports:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">      -</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;8081:8081&quot;</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    volumes:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">      -</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> /home/work/volumes/spark:/data</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    environment:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">      -</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;SPARK_MASTER=spark://spark-master:7077&quot;</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">  spark-worker-2:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    image:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> bde2020/spark-master:3.2.1-hadoop3.2</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    container_name:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> spark-worker-2</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    depends_on:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">      -</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> spark-master</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    ports:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">      -</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;8082:8082&quot;</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    volumes:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">      -</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> /home/work/volumes/spark:/data</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    environment:</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">      -</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;SPARK_MASTER=spark://spark-master:7077&quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在浏览器中输入<a href="http://172.16.185.176:8080/" target="_blank" rel="noopener noreferrer">http://172.16.185.176:8080/</a>就能看到<a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a><code>Master</code>节点的基本信息。</p><br><h2 id="本机独立部署" tabindex="-1"><a class="header-anchor" href="#本机独立部署"><span>本机独立部署</span></a></h2><p>下载并解压<a href="https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz" target="_blank" rel="noopener noreferrer">Spark安装包</a>。</p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" data-title="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; cd /home/work</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; tar zxvf spark-3.2.1-bin-hadoop3.2.tgz</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; mv spark-3.2.1-bin-hadoop3.2 spark-3.2.1</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; cd spark-3.2.1</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>修改配置文件<code>spark-env.sh</code>。</p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" data-title="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; cd conf</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; cp spark-env.sh.template spark-env.sh</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; vi spark-env.sh</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-dark:#7F848E;--shiki-light-font-style:italic;--shiki-dark-font-style:italic;"># 在文件末尾添加内容</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">export</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;"> JAVA_HOME</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">/</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">usr</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">/</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">local</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">/</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">java</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">/</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">jdk1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">8</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">0_401</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">export</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;"> SPARK_MASTER_HOST</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">hadoop</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>修改配置文件<code>workers</code>（在<a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a> <code>3.x</code>版本中，已经把文件<code>slaves</code>的名字改为了<code>workers</code>）。</p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" data-title="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; cp workers.template workers</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; vi workers</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">hadoop</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>启动<a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a>。</p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" data-title="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; cd /home/work/spark-3.2.1</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; ./sbin/start-all.sh</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>如果启动成功，就能用<code>jps</code>命令看到<code>Master</code>和<code>Worker</code>这两个进程。</p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" data-title="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; jps</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">171873</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> Jps</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">171820</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> Worker</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">171709</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> Master</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在浏览器中输入<a href="http://172.16.185.176:8080/" target="_blank" rel="noopener noreferrer">http://172.16.185.176:8080/</a>就能看到<a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a><code>Master</code>节点的基本信息。</p><br><h2 id="提交测试任务" tabindex="-1"><a class="header-anchor" href="#提交测试任务"><span>提交测试任务</span></a></h2><p>官方给出了比较详细的文档来说明<a href="https://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener noreferrer">如何向Spark提交计算任务</a>，照着它里面给出的提示来做就行了。</p><p>例如，向<a href="https://spark.apache.org/" target="_blank" rel="noopener noreferrer">Spark</a>提交一个<code>PI计算</code>任务。</p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" data-title="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&gt; ./bin/spark-submit </span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">\\</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">    --class</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> org.apache.spark.examples.SparkPi</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">    --master</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> spark://hadoop:7077</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>
<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">    examples/jars/spark-examples_2.12-3.2.1.jar</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> \\</span></span>
<span class="line"><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">    10</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>运行后得到结果如下。</p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" data-title="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">......</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">24/07/17</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> 15:42:08</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> INFO</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> DAGScheduler:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> ResultStage</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> (reduce </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">at</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> SparkPi.scala:38</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) finished in 44.468 s</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">24/07/17</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> 15:42:08</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> INFO</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> DAGScheduler:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> Job</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> is</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> finished.</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> Cancelling</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> potential</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> speculative</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> or</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> zombie</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> tasks</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> for</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> this</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> job</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">24/07/17</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> 15:42:08</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> INFO</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> TaskSchedulerImpl:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> Killing</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> all</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> running</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> tasks</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> in</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> stage</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> 0:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> Stage</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> finished</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">24/07/17</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> 15:42:08</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> INFO</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> DAGScheduler:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> Job</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> finished:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> reduce</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> at</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> SparkPi.scala:38,</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> took</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 44.574533</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> s</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">Pi</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> is</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> roughly</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 3.1395231395231393</span></span>
<span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">24/07/17</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> 15:42:08</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> INFO</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> SparkUI:</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> Stopped</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> Spark</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> web</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> UI</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> at</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> http://hadoop:4040</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">......</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,39),r=[l];function t(p,h){return i(),a("div",null,r)}const o=s(n,[["render",t],["__file","index.html.vue"]]),c=JSON.parse('{"path":"/technology/bigdata/spark/","title":"Spark实时大数据系统","lang":"zh-CN","frontmatter":{"title":"Spark实时大数据系统","icon":"fire","category":["大数据","Spark"],"tag":["大数据","Spark"],"date":"2023-04-07T00:00:00.000Z","isOriginal":true,"star":true,"description":"Spark概述 如果说Hadoop是用来做分布式数据存储的话，那么Spark就是是用来做分布式数据计算的。 虽然MapReduce也是分布式数据计算引擎，但它基本上都是通过磁盘执行计算任务的。 而Spark完全是基于内存的计算引擎，所以它不仅可以完成和MapReduce同样的离线计算任务，还能执行实时计算任务，以及和Hive类似的SQL查询。 所以Sp...","head":[["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/technology/bigdata/spark/"}],["meta",{"property":"og:site_name","content":"添码座"}],["meta",{"property":"og:title","content":"Spark实时大数据系统"}],["meta",{"property":"og:description","content":"Spark概述 如果说Hadoop是用来做分布式数据存储的话，那么Spark就是是用来做分布式数据计算的。 虽然MapReduce也是分布式数据计算引擎，但它基本上都是通过磁盘执行计算任务的。 而Spark完全是基于内存的计算引擎，所以它不仅可以完成和MapReduce同样的离线计算任务，还能执行实时计算任务，以及和Hive类似的SQL查询。 所以Sp..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://tianmazuo.com/technology/bigdata/spark/spark-01.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"添码座"}],["meta",{"property":"article:tag","content":"大数据"}],["meta",{"property":"article:tag","content":"Spark"}],["meta",{"property":"article:published_time","content":"2023-04-07T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Spark实时大数据系统\\",\\"image\\":[\\"https://tianmazuo.com/technology/bigdata/spark/spark-01.png\\"],\\"datePublished\\":\\"2023-04-07T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"添码座\\",\\"url\\":\\"https://www.tianmazuo.com/about/\\"}]}"]]},"headers":[{"level":2,"title":"Spark概述","slug":"spark概述","link":"#spark概述","children":[]},{"level":2,"title":"Docker独立部署","slug":"docker独立部署","link":"#docker独立部署","children":[]},{"level":2,"title":"本机独立部署","slug":"本机独立部署","link":"#本机独立部署","children":[]},{"level":2,"title":"提交测试任务","slug":"提交测试任务","link":"#提交测试任务","children":[]}],"git":{},"readingTime":{"minutes":4.03,"words":1208},"filePathRelative":"technology/bigdata/spark/README.md","localizedDate":"2023年4月7日","excerpt":"<h2>Spark概述</h2>\\n<p>如果说<a href=\\"https://hadoop.apache.org\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Hadoop</a>是用来做分布式数据存储的话，那么<a href=\\"https://spark.apache.org/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Spark</a>就是是用来做分布式数据计算的。</p>\\n<p>虽然<a href=\\"https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">MapReduce</a>也是分布式数据计算引擎，但它基本上都是通过磁盘执行计算任务的。</p>","autoDesc":true}');export{o as comp,c as data};
