import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,f as i,d as e,o as n}from"./app-B5m7CcSj.js";const r={},o=e('<p>相对于文生文、文生图这两类，视频领域的AI类应用就少多了。</p><figure><img src="https://tianmazuo.com/application/ai/t2v-01.png" alt="视频领域的AI类应用" tabindex="0" loading="lazy"><figcaption>视频领域的AI类应用</figcaption></figure><p>目前在文生视频领域，领跑的是<a href="https://openai.com/index/sora/" target="_blank" rel="noopener noreferrer">Sora</a>和<a href="https://www.stablevideo.com/" target="_blank" rel="noopener noreferrer">Stable Video</a>，以及曾经默默无闻的<a href="https://app.runwayml.com/video-tools/teams/xiangwang/dashboard" target="_blank" rel="noopener noreferrer">Runway</a>，不知道为什么<a href="https://www.midjourney.com/" target="_blank" rel="noopener noreferrer">Midjourney</a>反而没声响了。</p><p>不过，虽然<a href="https://openai.com/index/sora/" target="_blank" rel="noopener noreferrer">Sora</a>动静闹的挺大，但是至今未下一滴雨——到现在都只有官方的样例，试用的入口迟迟不出现——只能看，不能摸！</p><p>被媒体炒得火热的<a href="https://pw.shengshu-ai.com/video" target="_blank" rel="noopener noreferrer">Vidu</a>（<code>清华版Sora</code>），其实是清华联合<a href="https://www.shengshu-ai.com/home" target="_blank" rel="noopener noreferrer">生数科技</a>搞出来的。</p><figure><img src="https://tianmazuo.com/application/ai/vidu-text2video-01.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>截至本文发布为止，<a href="https://pw.shengshu-ai.com/video" target="_blank" rel="noopener noreferrer">Vidu</a>仍旧无法使用，难不成非得等着<a href="https://openai.com/index/sora" target="_blank" rel="noopener noreferrer">Sora</a>先出来？</p><br><h2 id="stable-video" tabindex="-1"><a class="header-anchor" href="#stable-video"><span>Stable Video</span></a></h2><p>这里拿之前用<a href="https://en.wikipedia.org/wiki/Artificial_intelligence" target="_blank" rel="noopener noreferrer">AI</a>拓词的<code>一个疯狂内卷的中国IT工程师</code>来让<a href="https://www.stablevideo.com/" target="_blank" rel="noopener noreferrer">Stable Video</a>创建视频。</p><div class="hint-container info"><p class="hint-container-title">AI拓词的内容</p><p>best quality,4k,8k,highres,masterpiece::1.5,ultra-detailed,realistic,photorealistic,photo-realistic:1.37,portrait,concept artist,China,IT engineer,crazy,inside,Chinese,computer,office,technology,chaotic,work,overwork,exhausted,coffee,keyboard,mouse,screen,code,program,deadline,pressure,stress,competition,ambition,hardworking,innovation,modern,urban,city,night,cityscape,neon lights,glow,reflection,window,desk,chair,tired,eye bags,coffee stain,fast-paced,energetic,high-tech,rapid development,innovative,creative,mind-blowing,competitive spirit,innovation,efficiency,modern lifestyle,asian,East Asian,traditional,modern clash,hectic pace,blurry motion,late night,shadow,artificial light,computer-generated,scene,man,2man,working,typing,scrolling,monitor,multitasking,sleepy,caffeine,technology overload,deadline-driven,tech-savvy,ambitious,competitive,hardworking,overworked,overwhelmed,stressful,fast-paced,highly competitive,modern office,night shift,modern life,fast-paced lifestyle,workaholic,innovator,IT industry,Chinese culture,modern China,software development,innovative spirit,competition-driven,crazy competition,technological advancement,urban environment,fast-growing,modern workplace,tech industry,nighttime hustle,deadline pressure,overwhelming workload,innovative mindset,asian work culture,rapid technological change</p></div><br><p>用<a href="https://www.stablevideo.com/" target="_blank" rel="noopener noreferrer">Stable Video</a>创建视频的过程如下所示：</p><p>在以文生视频的地方，输入之前的<a href="https://en.wikipedia.org/wiki/Artificial_intelligence" target="_blank" rel="noopener noreferrer">AI</a>拓词内容。</p><figure><img src="https://tianmazuo.com/application/ai/stable-diffusion-text2video-01.png" alt="在以文生视频的地方，输入之前的AI拓词内容" tabindex="0" loading="lazy"><figcaption>在以文生视频的地方，输入之前的AI拓词内容</figcaption></figure><figure><img src="https://tianmazuo.com/application/ai/stable-diffusion-text2video-02.png" alt="Stable Video会先创建四张图片供选择" tabindex="0" loading="lazy"><figcaption><a href="https://www.stablevideo.com/" target="_blank" rel="noopener noreferrer">Stable Video</a>会先创建四张图片供选择</figcaption></figure><p>如果想画面更灵动，更有交互感，那么需要仔细设置镜头、机位、运行轨迹、放大缩小等。</p><figure><img src="https://tianmazuo.com/application/ai/stable-diffusion-text2video-03.png" alt="选择一种风格之后，再确定更多属性，然后继续创建视频" tabindex="0" loading="lazy"><figcaption>选择一种风格之后，再确定更多属性，然后继续创建视频</figcaption></figure><p>正在创建视频。</p><figure><img src="https://tianmazuo.com/application/ai/stable-diffusion-text2video-04.png" alt="创建视频" tabindex="0" loading="lazy"><figcaption>创建视频</figcaption></figure><p>创建完成后对结果进行预览。</p><figure><img src="https://tianmazuo.com/application/ai/stable-diffusion-text2video-05.png" alt="创建结果预览" tabindex="0" loading="lazy"><figcaption>创建结果预览</figcaption></figure><p>虽然生成了视频，但是可以看出来，<a href="https://www.stablevideo.com/" target="_blank" rel="noopener noreferrer">Stable Video</a>依然有很大的瑕疵。</p><ul><li><p>从预览可以看到人物的面部明显有扭曲。</p></li><li><p>所有的视频都只有4秒钟，远远不能满足正常视频的长度要求。</p></li></ul><p>我用<a href="https://www.stablevideo.com/" target="_blank" rel="noopener noreferrer">Stable Video</a>生成的视频在这里：<a href="https://www.stablevideo.com/generate/a267ff20-6ff9-4634-9863-63991e196163" target="_blank" rel="noopener noreferrer">点击观看</a>。</p><p>这是我用<a href="https://stability.ai/" target="_blank" rel="noopener noreferrer">Stable Diffusion</a>创作的图生视频（忘了截图😢)。</p><figure><img src="https://tianmazuo.com/application/ai/stable-diffusion-text2video-06.gif" alt="Stable Diffusion的图生视频" tabindex="0" loading="lazy"><figcaption><a href="https://stability.ai/" target="_blank" rel="noopener noreferrer">Stable Diffusion</a>的图生视频</figcaption></figure><br><h2 id="runway" tabindex="-1"><a class="header-anchor" href="#runway"><span>Runway</span></a></h2><p>据说<a href="https://app.runwayml.com/video-tools/teams/xiangwang/dashboard" target="_blank" rel="noopener noreferrer">Runway</a>是<a href="https://www.stablevideo.com/" target="_blank" rel="noopener noreferrer">Stable Video</a>背后的技术公司。</p><p>不过其视频生成模型从2023年3月直到现在仍然还是<code>Gen-2</code>，都有近一年的时间没有升级过了。</p><p>既然<a href="https://www.stablevideo.com/" target="_blank" rel="noopener noreferrer">Stable Video</a>用的是它的技术，那么<a href="https://www.stablevideo.com/" target="_blank" rel="noopener noreferrer">Stable Diffusion</a>生成视频的问题，它肯定也会有。</p><p>选择Gen-2大模型。</p><figure><img src="https://tianmazuo.com/application/ai/runway-text2video-01.png" alt="选择Gen-2大模型" tabindex="0" loading="lazy"><figcaption>选择Gen-2大模型</figcaption></figure><p>输入提示词。</p><figure><img src="https://tianmazuo.com/application/ai/runway-text2video-02.png" alt="输入提示词" tabindex="0" loading="lazy"><figcaption>输入提示词</figcaption></figure><p>果然是师出同门，连时长限制都一样。</p><figure><img src="https://tianmazuo.com/application/ai/runway-text2video-03.png" alt="连时长限制都一样" tabindex="0" loading="lazy"><figcaption>连时长限制都一样</figcaption></figure><p>而且同样存在画面扭曲、错乱问题。</p><figure><img src="https://tianmazuo.com/application/ai/runway-text2video-04.png" alt="画面扭曲、错乱" tabindex="0" loading="lazy"><figcaption>画面扭曲、错乱</figcaption></figure><p>即使是时间延长，看起来也好像没改一样。</p><figure><img src="https://tianmazuo.com/application/ai/runway-text2video-05.png" alt="时间延长后的效果" tabindex="0" loading="lazy"><figcaption>时间延长后的效果</figcaption></figure><p>我用<a href="https://app.runwayml.com/video-tools/teams/xiangwang/dashboard" target="_blank" rel="noopener noreferrer">Runway</a>生成的视频在这里：<a href="https://app.runwayml.com/creation/9a463d50-73e2-4e19-b62d-b1aecc908f7d" target="_blank" rel="noopener noreferrer">点击观看</a>。</p><br><h2 id="comfyui" tabindex="-1"><a class="header-anchor" href="#comfyui"><span>ComfyUI</span></a></h2><p>因为目前<a href="https://openai.com/index/sora" target="_blank" rel="noopener noreferrer">Sora</a>还没放开，<a href="https://www.stablevideo.com/" target="_blank" rel="noopener noreferrer">Stable Video</a>和<a href="https://app.runwayml.com/video-tools/teams/xiangwang/dashboard" target="_blank" rel="noopener noreferrer">Runway</a>又太拉胯，所以<a href="https://github.com/comfyanonymous/ComfyUI" target="_blank" rel="noopener noreferrer">ComfyUI</a> + <a href="https://github.com/Stability-AI/generative-models" target="_blank" rel="noopener noreferrer">SVD</a>就成了另一个不错的替代选择。</p><p>以<a href="https://comfyuiweb.com/#ai-image-generator" target="_blank" rel="noopener noreferrer">ComfyUI Web</a>为例，进入时先清空所有的工作流，准备安装自定义节点和模型。</p><div style="text-align:center;"><p><img src="https://tianmazuo.com/application/ai/comfyui-text2video-01.png" alt="先清空所有的工作流" width="500" height="350" loading="lazy"> <img src="https://tianmazuo.com/application/ai/comfyui-text2video-02.png" alt="准备安装自定义节点和模型" width="500" height="350" loading="lazy"></p></div><div style="text-align:center;"><p><img src="https://tianmazuo.com/application/ai/comfyui-text2video-03.png" alt="安装自定义节点" width="500" height="350" loading="lazy"> <img src="https://tianmazuo.com/application/ai/comfyui-text2video-04.png" alt="安装SVD大模型" width="500" height="350" loading="lazy"></p></div><div style="text-align:center;"><p><img src="https://tianmazuo.com/application/ai/comfyui-text2video-05.png" alt="选择自定义节点" width="500" height="350" loading="lazy"> <img src="https://tianmazuo.com/application/ai/comfyui-text2video-06.png" alt="选择自定义节点" width="500" height="350" loading="lazy"></p></div><p>之后再下载<a href="https://github.com/comfyanonymous/ComfyUI" target="_blank" rel="noopener noreferrer">ComfyUI</a>官方提供的<a href="https://comfyanonymous.github.io/ComfyUI_examples/video/workflow_image_to_video.json" target="_blank" rel="noopener noreferrer">工作流文件（JSON格式）</a>，或者是其他工作流文件来完成图生视频流程。</p><p>剩下的事情就是按照<a href="https://openart.ai/workflows/academy/" target="_blank" rel="noopener noreferrer">ComfyUI Academy</a>或<a href="https://github.com/Suzie1/ComfyUI_Guide_To_Making_Custom_Nodes/wiki" target="_blank" rel="noopener noreferrer">ComfyUI Guide To Making Custom Nodes</a>教程一步步地生成视频，这里就不再赘述了。</p><p>有一点需要注意的是：只有颜色相同的端点（或同类型的端点）之间才能连线。</p><figure><img src="https://tianmazuo.com/application/ai/comfyui-text2video-07.png" alt="只有颜色相同的端点之间才能连线" tabindex="0" loading="lazy"><figcaption>只有颜色相同的端点之间才能连线</figcaption></figure><p>不过，如果<a href="https://stability.ai/" target="_blank" rel="noopener noreferrer">Stable Diffusion</a>或者<a href="https://comfyuiweb.com/#ai-image-generator" target="_blank" rel="noopener noreferrer">ComfyUI Web</a>玩得够溜得话，是可以创造出下面这些非常惊艳的作品的（当然这对计算机的配置有要求）。</p><div style="text-align:center;"><p><img src="https://tianmazuo.com/application/ai/comfyui-text2video-08.gif" alt="ComfyUI Web生成的视频" loading="lazy">        <img src="https://tianmazuo.com/application/ai/comfyui-text2video-09.gif" alt="ComfyUI Web生成的视频" loading="lazy"></p></div><div style="text-align:center;"><p><img src="https://tianmazuo.com/application/ai/comfyui-text2video-10.gif" alt="ComfyUI Web生成的视频" loading="lazy"> <img src="https://tianmazuo.com/application/ai/comfyui-text2video-11.gif" alt="ComfyUI Web生成的视频" loading="lazy"></p></div><br><h2 id="pika" tabindex="-1"><a class="header-anchor" href="#pika"><span>Pika</span></a></h2><p><a href="https://pika.art/home" target="_blank" rel="noopener noreferrer">Pika</a>也是一个<a href="https://en.wikipedia.org/wiki/Artificial_intelligence" target="_blank" rel="noopener noreferrer">AI</a>视频生成工具，它可以生成3D动画、动漫、卡通和电影风格的视频，并且有强大的视频编辑功能，如画布延展、局部修改、视频时长拓展等。</p><figure><img src="https://tianmazuo.com/application/ai/pika-text2video-01.png" alt="创建视频" tabindex="0" loading="lazy"><figcaption>创建视频</figcaption></figure><div class="hint-container info"><p class="hint-container-title">完整的提示词</p><p>Unreal engine, 3D render, At dusk on the beach, the sea waves are surging, with pure white and soft sand on the beach and orange red clouds in the sky. The goddess wears a red dress and rides on a tall and muscular white horse, holding a reins in her hand, smiling and looking into the distance. The afterglow of the sunset shines on the goddess and horses, casting soft and warm light and shadow, making the entire scene more beautiful and romantic. With white as the main tone, combined with light yellow</p><ul><li>下面的部分因为超长被Pika截断了</li></ul><p>, light blue, deep brown and other tones, it presents a fresh, natural, and lightweight feeling. Filled with a free, open-minded, and carefree atmosphere, it makes people feel comfortable, joyful, and relaxed. Using a close range side view, the beautiful lines and muscular lines of the goddess and horse are highlighted, enhancing the dynamic and aesthetic feel of the entire scene. By combining elements of realism and romanticism, a beautiful, elegant, and dreamy artistic style is created, allowing people to feel the nobility and elegance of goddesses and horses. --v 5.2 --s 100 --ar 1:1 --c 0 --q 1</p></div><p>我用<a href="https://pika.art/home" target="_blank" rel="noopener noreferrer">Pika</a>生成的视频在这里：<a href="https://pika.art/video/2a6c9d13-bd3b-4a88-9478-e6d724f16c77" target="_blank" rel="noopener noreferrer">点击观看</a>。</p><p>从效果来看，貌似用的也是<a href="https://app.runwayml.com/video-tools/teams/xiangwang/dashboard" target="_blank" rel="noopener noreferrer">Runway</a>的技术😢。</p><p>顺便说一句，上面这段提示词，我用<a href="https://www.midjourney.com/" target="_blank" rel="noopener noreferrer">Midjourney</a><code>画</code>，其中的两张是这样的。</p><div style="text-align:center;"><p><img src="https://tianmazuo.com/application/ai/midjourney-01.png" alt="海边的女神" width="500" height="500" loading="lazy"> <img src="https://tianmazuo.com/application/ai/midjourney-02.png" alt="海边的女神" width="500" height="500" loading="lazy"></p></div><br><h2 id="streamingt2v" tabindex="-1"><a class="header-anchor" href="#streamingt2v"><span>StreamingT2V</span></a></h2><p>根据<a href="https://github.com/" target="_blank" rel="noopener noreferrer">Github</a>上的介绍， <a href="https://github.com/Picsart-AI-Research/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V</a>是一种先进的自回归技术，可以创建具有丰富运动动态且没有任何停滞的长视频。视频可以达到1200帧、时长2分钟，并且可以延长更长的持续时间。</p><p>这是它生成的视频。</p><div style="text-align:center;"><iframe src="https://streamingt2v.github.io/static/videos/1200/0005_0000_Wide_shot_of_battlefield,_stormtroopers_.mp4" width="500" height="400" align="center"></iframe>         <iframe src="https://streamingt2v.github.io/static/videos/600/0000_0000_Camera_following_a_pack_of_crows_flying_.mp4" width="500" height="400" align="center"></iframe></div>',71),l=e('<br><p><a href="https://github.com/Picsart-AI-Research/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V</a>一共有三种模型（或者说模式）可以选择。</p><ul><li><a href="https://github.com/modelscope/modelscope" target="_blank" rel="noopener noreferrer">ModelscopeT2V</a>：文生图。</li></ul><table><thead><tr><th style="text-align:center;">帧数</th><th style="text-align:center;">更快预览的推理时间 (256x256)</th><th style="text-align:center;">最终结果的推理时间 (720x720)</th></tr></thead><tbody><tr><td style="text-align:center;">24帧</td><td style="text-align:center;">40秒</td><td style="text-align:center;">165秒</td></tr><tr><td style="text-align:center;">56帧</td><td style="text-align:center;">75秒</td><td style="text-align:center;">360秒</td></tr><tr><td style="text-align:center;">80帧</td><td style="text-align:center;">110秒</td><td style="text-align:center;">525秒</td></tr><tr><td style="text-align:center;">240帧</td><td style="text-align:center;">340秒</td><td style="text-align:center;">1610秒</td></tr><tr><td style="text-align:center;">600帧</td><td style="text-align:center;">860秒</td><td style="text-align:center;">5128秒</td></tr><tr><td style="text-align:center;">1200帧</td><td style="text-align:center;">1710秒</td><td style="text-align:center;">10225秒</td></tr></tbody></table><ul><li><a href="https://github.com/guoyww/AnimateDiff" target="_blank" rel="noopener noreferrer">AnimateDiff</a>：文生图。</li></ul><table><thead><tr><th style="text-align:center;">帧数</th><th style="text-align:center;">更快预览的推理时间 (256x256)</th><th style="text-align:center;">最终结果的推理时间 (720x720)</th></tr></thead><tbody><tr><td style="text-align:center;">24帧</td><td style="text-align:center;">50秒</td><td style="text-align:center;">180秒</td></tr><tr><td style="text-align:center;">56帧</td><td style="text-align:center;">85秒</td><td style="text-align:center;">370秒</td></tr><tr><td style="text-align:center;">80帧</td><td style="text-align:center;">120秒</td><td style="text-align:center;">535秒</td></tr><tr><td style="text-align:center;">240帧</td><td style="text-align:center;">350秒</td><td style="text-align:center;">1620秒</td></tr><tr><td style="text-align:center;">600帧</td><td style="text-align:center;">870秒</td><td style="text-align:center;">5138秒</td></tr><tr><td style="text-align:center;">1200帧</td><td style="text-align:center;">1720秒</td><td style="text-align:center;">10235秒</td></tr></tbody></table><ul><li><a href="https://github.com/Stability-AI/generative-models" target="_blank" rel="noopener noreferrer">SVD</a>：图生图。</li></ul><table><thead><tr><th style="text-align:center;">帧数</th><th style="text-align:center;">更快预览的推理时间 (256x256)</th><th style="text-align:center;">最终结果的推理时间 (720x720)</th></tr></thead><tbody><tr><td style="text-align:center;">24帧</td><td style="text-align:center;">80秒</td><td style="text-align:center;">210秒</td></tr><tr><td style="text-align:center;">56帧</td><td style="text-align:center;">115秒</td><td style="text-align:center;">400秒</td></tr><tr><td style="text-align:center;">80帧</td><td style="text-align:center;">150秒</td><td style="text-align:center;">565秒</td></tr><tr><td style="text-align:center;">240帧</td><td style="text-align:center;">380秒</td><td style="text-align:center;">1650秒</td></tr><tr><td style="text-align:center;">600帧</td><td style="text-align:center;">900秒</td><td style="text-align:center;">5168秒</td></tr><tr><td style="text-align:center;">1200帧</td><td style="text-align:center;">1750秒</td><td style="text-align:center;">10265秒</td></tr></tbody></table><br><p>现在，用之前给<a href="https://pika.art/home" target="_blank" rel="noopener noreferrer">Pika</a>的<a href="https://platform.openai.com/docs/guides/prompt-engineering" target="_blank" rel="noopener noreferrer">提示词</a>，再用<a href="https://github.com/Picsart-AI-Research/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V</a>来生成一次视频。</p><div class="hint-container info"><p class="hint-container-title">提示词</p><p>Unreal engine, 3D render, At dusk on the beach, the sea waves are surging, with pure white and soft sand on the beach and orange red clouds in the sky. The goddess wears a red dress and rides on a tall and muscular white horse, holding a reins in her hand, smiling and looking into the distance. The afterglow of the sunset shines on the goddess and horses, casting soft and warm light and shadow, making the entire scene more beautiful and romantic. With white as the main tone, combined with light yellow, light blue, deep brown and other tones, it presents a fresh, natural, and lightweight feeling. Filled with a free, open-minded, and carefree atmosphere, it makes people feel comfortable, joyful, and relaxed. Using a close range side view, the beautiful lines and muscular lines of the goddess and horse are highlighted, enhancing the dynamic and aesthetic feel of the entire scene. By combining elements of realism and romanticism, a beautiful, elegant, and dreamy artistic style is created, allowing people to feel the nobility and elegance of goddesses and horses.</p></div><br><p>可以在这里试试<a href="https://github.com/Picsart-AI-Research/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V</a>是不是真像它说的那样强大：<a href="https://huggingface.co/spaces/PAIR/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V在线试玩</a></p><figure><img src="https://tianmazuo.com/application/ai/streamingt2v-text2video-01.png" alt="用StreamingT2V文生视频界面" tabindex="0" loading="lazy"><figcaption>用<a href="https://github.com/Picsart-AI-Research/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V</a>文生视频界面</figcaption></figure><p>这里用的是<code>ModelscopeT2V</code>文生图模型，并且选的帧数是32帧。</p><p><strong><mark>文生视频</mark></strong> 的结果。</p><ul><li><p>用<a href="https://github.com/Picsart-AI-Research/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V</a>文生视频生成的临时结果：<a href="https://pair-streamingt2v.hf.space/file=/tmp/gradio/c461096f9f8ae98e35b68e559668dacd79b459a2/Unreal_engine_3D_render_At_dusk_on_the_beach_the_sea_waves_are_surging_with_pure_white_and_soft__16_42_58_362230.mp4" target="_blank" rel="noopener noreferrer">点击观看</a></p></li><li><p>用<a href="https://github.com/Picsart-AI-Research/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V</a>文生视频生成的最终结果：<a href="https://pair-streamingt2v.hf.space/file=/tmp/gradio/bebb857d517b01d24042dd444d4607fb44e57fd1/Unreal_engine_3D_render_At_dusk_on_the_beach_the_sea_waves_are_surging_with_pure_white_and_soft__17_05_29_662296_enhanced.mp4" target="_blank" rel="noopener noreferrer">点击观看</a></p></li></ul><p>不知道为什么最终的<code>长</code>视频只有3秒😢，也许是提示词<code>太复杂</code>了。</p><hr><p>然后，将帧数调整为24帧，再用下面这幅图片，以<a href="https://github.com/Stability-AI/generative-models" target="_blank" rel="noopener noreferrer">SVD</a>模型再生成一次。</p><figure><img src="https://tianmazuo.com/application/ai/midjourney-02.png" alt="海边的女神" width="500" height="500" tabindex="0" loading="lazy"><figcaption>海边的女神</figcaption></figure><p><strong><mark>图生视频</mark></strong> 的结果。</p><ul><li><p>用<a href="https://github.com/Picsart-AI-Research/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V</a>图生视频生成的临时结果：<a href="https://pair-streamingt2v.hf.space/file=/tmp/gradio/f1364cc35f233231633581ef81aa8e75e27afd75/Unreal_engine_3D_render_At_dusk_on_the_beach_the_sea_waves_are_surging_with_pure_white_and_soft__17_32_43_320378.mp4" target="_blank" rel="noopener noreferrer">点击观看</a></p></li><li><p>用<a href="https://github.com/Picsart-AI-Research/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V</a>图生视频生成的最终结果：<a href="https://pair-streamingt2v.hf.space/file=/tmp/gradio/dad79820a1c90b20b5fe73ef7cebfc6b8ada29d4/Unreal_engine_3D_render_At_dusk_on_the_beach_the_sea_waves_are_surging_with_pure_white_and_soft__17_50_19_834408_enhanced.mp4" target="_blank" rel="noopener noreferrer">点击观看</a></p></li></ul><br><h2 id="text2video-zero" tabindex="-1"><a class="header-anchor" href="#text2video-zero"><span>Text2Video-Zero</span></a></h2><p><a href="https://huggingface.co/spaces/PAIR/Text2Video-Zero" target="_blank" rel="noopener noreferrer">Text2Video-Zero</a>其实和以上这些文生视频模型很类似，其界面几乎和<a href="https://github.com/Picsart-AI-Research/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V</a>一模一样，一看就是<a href="https://stability.ai/" target="_blank" rel="noopener noreferrer">Stable Diffusion</a>风格的。</p><p>只不过它可选择的模型更多，且能够输入负向<a href="https://platform.openai.com/docs/guides/prompt-engineering" target="_blank" rel="noopener noreferrer">提示词</a>。</p><figure><img src="https://tianmazuo.com/application/ai/text2video-zero-text2video-01.png" alt="Text2Video-Zero界面" tabindex="0" loading="lazy"><figcaption>Text2Video-Zero界面</figcaption></figure><p>可以把刚才给<a href="https://github.com/Picsart-AI-Research/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V</a>用的<a href="https://platform.openai.com/docs/guides/prompt-engineering" target="_blank" rel="noopener noreferrer">提示词</a>再给它用一次看看效果。</p><p>我用<a href="https://huggingface.co/spaces/PAIR/Text2Video-Zero" target="_blank" rel="noopener noreferrer">Text2Video-Zero</a>生成的视频在这里：<a href="https://pair-text2video-zero.hf.space/file=/tmp/ada0c09cf3ecda5a8966f498c1193cb27349a800/movie.mp4" target="_blank" rel="noopener noreferrer">点击观看</a>。</p><p>感觉完全不是那回事。</p><p>纵观<a href="https://github.com/Picsart-AI-Research/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V</a>和<a href="https://huggingface.co/spaces/PAIR/Text2Video-Zero" target="_blank" rel="noopener noreferrer">Text2Video-Zero</a>表现，也许是参数不够，也许是<a href="https://platform.openai.com/docs/guides/prompt-engineering" target="_blank" rel="noopener noreferrer">提示词</a>要调整，或者是<code>GPU</code>不够强劲。</p><p>不管如何，用官方搭建的环境试玩后，效果差强人意。</p><br><h2 id="剪映" tabindex="-1"><a class="header-anchor" href="#剪映"><span>剪映</span></a></h2><p>其实，要说到生成视频，有些工具软件也非常好用，例如，国内短视频博主们基本上都知道的<a href="https://www.capcut.cn/" target="_blank" rel="noopener noreferrer">剪映</a>。</p><p>在<a href="https://chatgpt.com/" target="_blank" rel="noopener noreferrer">ChatGPT</a>刚面世的2022年底，<a href="https://www.capcut.cn/" target="_blank" rel="noopener noreferrer">剪映</a>就已经有了<code>图文成片</code>功能，这其实就是现在的<code>文生视频</code>，虽然它是靠关键词搜索而来的。</p><p>以下展示了<a href="https://www.capcut.cn/" target="_blank" rel="noopener noreferrer">剪映</a>的图文成片功能。</p><figure><img src="https://tianmazuo.com/application/ai/jianying-text2video-01.png" alt="选择“图文成片”" tabindex="0" loading="lazy"><figcaption>选择“图文成片”</figcaption></figure><p>选择一个主题，并输入相关话题。</p><figure><img src="https://tianmazuo.com/application/ai/jianying-text2video-02.png" alt="选择一个主题，并输入相关话题" tabindex="0" loading="lazy"><figcaption>选择一个主题，并输入相关话题</figcaption></figure><p><a href="https://www.capcut.cn/" target="_blank" rel="noopener noreferrer">剪映</a>居然提供了多个拓词结果，甚至还可以选择配音和<code>BGM</code>。</p><figure><img src="https://tianmazuo.com/application/ai/jianying-text2video-03.png" alt="不但有多个拓词结果，甚至还可以选择配音" tabindex="0" loading="lazy"><figcaption>不但有多个拓词结果，甚至还可以选择配音</figcaption></figure><p>生成视频。</p><figure><img src="https://tianmazuo.com/application/ai/jianying-text2video-04.png" alt="生成视频" tabindex="0" loading="lazy"><figcaption>生成视频</figcaption></figure><p><a href="https://www.capcut.cn/" target="_blank" rel="noopener noreferrer">剪映</a>生成的视频工程文件，只要将它导出来，就是一段既有配音、又有字幕、还有BGM的视频了。</p><figure><img src="https://tianmazuo.com/application/ai/jianying-text2video-05.png" alt="生成的视频工程文件" tabindex="0" loading="lazy"><figcaption>生成的视频工程文件</figcaption></figure><p>当然，这种风格的视频和大模型生成的风格还是不一样的，但人家有任意时长、有配音、有字幕、有BGM，这可能也是大模型今后努力的方向吧。</p><br><h2 id="视频生文" tabindex="-1"><a class="header-anchor" href="#视频生文"><span>视频生文</span></a></h2><p>前面提到过的文生视频工具<a href="https://scenex.jina.ai/" target="_blank" rel="noopener noreferrer">SceneXplain</a>，它还可以反过来通过视频生成文本摘要，下面展示了这一过程。</p><p><a href="https://scenex.jina.ai/" target="_blank" rel="noopener noreferrer">SceneXplain</a>提供了几种不同的生成模式，我这里选择的是视频摘要。</p><div style="text-align:center;"><p><img src="https://tianmazuo.com/application/ai/textimage-scenexplain-07.png" alt="" width="500" height="500" loading="lazy"> <img src="https://tianmazuo.com/application/ai/textimage-scenexplain-08.png" alt="" width="500" height="500" loading="lazy"></p></div><p>貌似识别还是有点问题。</p><br><h2 id="在线试玩" tabindex="-1"><a class="header-anchor" href="#在线试玩"><span>在线试玩</span></a></h2><p><a href="https://huggingface.co/spaces/PAIR/StreamingT2V" target="_blank" rel="noopener noreferrer">StreamingT2V在线试玩</a></p><p><a href="https://huggingface.co/spaces/PAIR/Text2Video-Zero" target="_blank" rel="noopener noreferrer">Text2Video-Zero在线试玩</a></p>',58);function p(s,c){return n(),a("div",null,[o,i(`
<VidStack
  src="https://streamingt2v.github.io/static/videos/1200/0005_0000_Wide_shot_of_battlefield,_stormtroopers_.mp4"
/>
`),l])}const h=t(r,[["render",p],["__file","ti2v.html.vue"]]),m=JSON.parse('{"path":"/application/ai/ti2v.html","title":"文/图生视频","lang":"zh-CN","frontmatter":{"title":"文/图生视频","icon":"file-video","category":["应用","U-AIGC"],"tag":["应用","U-AIGC","AI视频"],"date":"2024-01-04T00:00:00.000Z","isOriginal":true,"star":true,"description":"相对于文生文、文生图这两类，视频领域的AI类应用就少多了。 视频领域的AI类应用视频领域的AI类应用 目前在文生视频领域，领跑的是Sora和Stable Video，以及曾经默默无闻的Runway，不知道为什么Midjourney反而没声响了。 不过，虽然Sora动静闹的挺大，但是至今未下一滴雨——到现在都只有官方的样例，试用的入口迟迟不出现——只能看...","head":[["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/application/ai/ti2v.html"}],["meta",{"property":"og:site_name","content":"添码座"}],["meta",{"property":"og:title","content":"文/图生视频"}],["meta",{"property":"og:description","content":"相对于文生文、文生图这两类，视频领域的AI类应用就少多了。 视频领域的AI类应用视频领域的AI类应用 目前在文生视频领域，领跑的是Sora和Stable Video，以及曾经默默无闻的Runway，不知道为什么Midjourney反而没声响了。 不过，虽然Sora动静闹的挺大，但是至今未下一滴雨——到现在都只有官方的样例，试用的入口迟迟不出现——只能看..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://tianmazuo.com/application/ai/t2v-01.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"添码座"}],["meta",{"property":"article:tag","content":"应用"}],["meta",{"property":"article:tag","content":"U-AIGC"}],["meta",{"property":"article:tag","content":"AI视频"}],["meta",{"property":"article:published_time","content":"2024-01-04T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"文/图生视频\\",\\"image\\":[\\"https://tianmazuo.com/application/ai/t2v-01.png\\",\\"https://tianmazuo.com/application/ai/vidu-text2video-01.png\\",\\"https://tianmazuo.com/application/ai/stable-diffusion-text2video-01.png\\",\\"https://www.stablevideo.com/\\",\\"https://tianmazuo.com/application/ai/stable-diffusion-text2video-03.png\\",\\"https://tianmazuo.com/application/ai/stable-diffusion-text2video-04.png\\",\\"https://tianmazuo.com/application/ai/stable-diffusion-text2video-05.png\\",\\"https://stability.ai/\\",\\"https://tianmazuo.com/application/ai/runway-text2video-01.png\\",\\"https://tianmazuo.com/application/ai/runway-text2video-02.png\\",\\"https://tianmazuo.com/application/ai/runway-text2video-03.png\\",\\"https://tianmazuo.com/application/ai/runway-text2video-04.png\\",\\"https://tianmazuo.com/application/ai/runway-text2video-05.png\\",\\"https://tianmazuo.com/application/ai/comfyui-text2video-01.png =500x350\\",\\"https://tianmazuo.com/application/ai/comfyui-text2video-02.png =500x350\\",\\"https://tianmazuo.com/application/ai/comfyui-text2video-03.png =500x350\\",\\"https://tianmazuo.com/application/ai/comfyui-text2video-04.png =500x350\\",\\"https://tianmazuo.com/application/ai/comfyui-text2video-05.png =500x350\\",\\"https://tianmazuo.com/application/ai/comfyui-text2video-06.png =500x350\\",\\"https://tianmazuo.com/application/ai/comfyui-text2video-07.png\\",\\"https://comfyuiweb.com/#ai-image-generator\\",\\"https://comfyuiweb.com/#ai-image-generator\\",\\"https://comfyuiweb.com/#ai-image-generator\\",\\"https://comfyuiweb.com/#ai-image-generator\\",\\"https://tianmazuo.com/application/ai/pika-text2video-01.png\\",\\"https://tianmazuo.com/application/ai/midjourney-01.png =500x500\\",\\"https://tianmazuo.com/application/ai/midjourney-02.png =500x500\\",\\"https://github.com/Picsart-AI-Research/StreamingT2V\\",\\"https://tianmazuo.com/application/ai/midjourney-02.png =500x500\\",\\"https://tianmazuo.com/application/ai/text2video-zero-text2video-01.png\\",\\"https://tianmazuo.com/application/ai/jianying-text2video-01.png\\",\\"https://tianmazuo.com/application/ai/jianying-text2video-02.png\\",\\"https://tianmazuo.com/application/ai/jianying-text2video-03.png\\",\\"https://tianmazuo.com/application/ai/jianying-text2video-04.png\\",\\"https://tianmazuo.com/application/ai/jianying-text2video-05.png\\",\\"https://tianmazuo.com/application/ai/textimage-scenexplain-07.png =500x500\\",\\"https://tianmazuo.com/application/ai/textimage-scenexplain-08.png =500x500\\"],\\"datePublished\\":\\"2024-01-04T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"添码座\\",\\"url\\":\\"https://www.tianmazuo.com/about/\\"}]}"]]},"headers":[{"level":2,"title":"Stable Video","slug":"stable-video","link":"#stable-video","children":[]},{"level":2,"title":"Runway","slug":"runway","link":"#runway","children":[]},{"level":2,"title":"ComfyUI","slug":"comfyui","link":"#comfyui","children":[]},{"level":2,"title":"Pika","slug":"pika","link":"#pika","children":[]},{"level":2,"title":"StreamingT2V","slug":"streamingt2v","link":"#streamingt2v","children":[]},{"level":2,"title":"Text2Video-Zero","slug":"text2video-zero","link":"#text2video-zero","children":[]},{"level":2,"title":"剪映","slug":"剪映","link":"#剪映","children":[]},{"level":2,"title":"视频生文","slug":"视频生文","link":"#视频生文","children":[]},{"level":2,"title":"在线试玩","slug":"在线试玩","link":"#在线试玩","children":[]}],"git":{},"readingTime":{"minutes":10.77,"words":3231},"filePathRelative":"application/ai/ti2v.md","localizedDate":"2024年1月4日","excerpt":"<p>相对于文生文、文生图这两类，视频领域的AI类应用就少多了。</p>\\n<figure><img src=\\"https://tianmazuo.com/application/ai/t2v-01.png\\" alt=\\"视频领域的AI类应用\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>视频领域的AI类应用</figcaption></figure>\\n<p>目前在文生视频领域，领跑的是<a href=\\"https://openai.com/index/sora/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Sora</a>和<a href=\\"https://www.stablevideo.com/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Stable Video</a>，以及曾经默默无闻的<a href=\\"https://app.runwayml.com/video-tools/teams/xiangwang/dashboard\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Runway</a>，不知道为什么<a href=\\"https://www.midjourney.com/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Midjourney</a>反而没声响了。</p>","autoDesc":true}');export{h as comp,m as data};
